import os
import sys
from datetime import datetime
from bs4 import BeautifulSoup, Tag
from typing import List, Dict, Optional, Any

class AthanorPurifier:
    """
    [ç‚¼é‡‘ç»„ä»¶]: ç‰©è´¨å‡€åŒ–å™¨ (V3 ç»ˆæç‰ˆ)
    é©±åŠ¨å¼•æ“: lxml (High-Performance Parser)
    ä½¿å‘½: é›¶é‡æ’ã€é›¶å™ªéŸ³ã€å…¨è‡ªåŠ¨ç¼–ç æ„Ÿåº”ï¼Œå®ç°ä¹¦ç­¾ä¿¡å·çš„ç»å¯¹èƒå–ã€‚
    """

    def __init__(self):
        # æ’é™¤æ— æ„ä¹‰çš„æ ¹èŠ‚ç‚¹ (Noise Reduction)
        self.noise_roots = {
            'Bookmarks', 'ä¹¦ç­¾', 'ä¹¦ç­¾æ ', 'æ”¶è—å¤¹', 'Bookmarks Bar', 
            'Bookmarks Menu', 'Personal Toolbar Folder', 'Other Bookmarks',
            'Mobile Bookmarks', 'ç§»åŠ¨ä¹¦ç­¾', 'Unfiled Bookmarks', 'æœªåˆ†ç±»ä¹¦ç­¾'
        }

    def _normalize_timestamp(self, ts_str: Optional[str]) -> str:
        """
        æ—¶é—´æˆ³å®šæ ‡ (Temporal Calibration)
        é˜ˆå€¼å®šä½äº 10^12ï¼Œå®Œç¾å…¼å®¹ Webkit (å¾®ç§’) ä¸ Unix (ç§’)ã€‚
        """
        if not ts_str:
            return ""
        try:
            ts = int(ts_str)
            # ä¿®æ­£é€»è¾‘ï¼šWebkit æ—¶é—´æˆ³é€šå¸¸ä¸º 17 ä½ï¼ŒUnix ä¸º 10 ä½
            if ts > 10**12:
                ts = ts / 1_000_000
            return datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
        except (ValueError, TypeError, OverflowError):
            return ""

    def _extract_context(self, link: Tag) -> List[str]:
        """é€†æµè€Œä¸Šï¼šä» A æ ‡ç­¾æ”€çˆ¬ DOM æ ‘ï¼Œæå–çŸ¥è¯†è¯­å¢ƒ"""
        context: List[str] = []
        if not isinstance(link, Tag):
            return context

        for parent in link.parents:
            if parent.name == 'dl':
                # å¯»æ‰¾ DL å®¹å™¨ä¹‹å‰çš„æ–‡ä»¶å¤¹æ ‡é¢˜ (H3 ä¸ºæ ‡å‡† Netscape æ ¼å¼æ ‡é¢˜)
                header = parent.find_previous_sibling(['h3', 'h1'])
                if isinstance(header, Tag):
                    name = header.get_text().strip()
                    if name and name not in self.noise_roots:
                        context.insert(0, name)
        return context

    def smelt(self, raw_content: bytes) -> List[Dict[str, Any]]:
        """
        æ ¸å¿ƒç†”ç‚¼é€»è¾‘ï¼šé«˜æ€§èƒ½å­—èŠ‚æµå¤„ç†ã€‚
        """
        # âš¡ å¼•æ“åˆ‡æ¢ï¼šä½¿ç”¨ 'html.parser' æ›¿ä»£ 'lxml'
        # ä¹¦ç­¾æ–‡ä»¶ (Netscape æ ¼å¼) å¾€å¾€åµŒå¥—ææ·±ä¸”ä¸è§„èŒƒï¼Œlxml è¿‡äºä¸¥æ ¼ä¼šå¯¼è‡´æ•°æ®æˆªæ–­ã€‚
        # html.parser è™½ç„¶æ…¢ä¸€ç‚¹ï¼Œä½†å®¹é”™æ€§æå¼ºï¼Œèƒ½ä¿è¯æ•°æ®çš„å®Œæ•´æ€§ (High Recall)ã€‚
        soup = BeautifulSoup(raw_content, 'html.parser', from_encoding=None)
        
        actual_encoding = soup.original_encoding
        if actual_encoding:
            print(f"ğŸ“¡  [Spectral Analysis] æ¢æµ‹åˆ°ç‰©è´¨ç¼–ç : {actual_encoding}")
            
        purified_data: List[Dict[str, Any]] = []
        raw_links = soup.find_all('a')
        
        for link in raw_links:
            if not isinstance(link, Tag):
                continue

            url = link.get('href', '')
            # è¿‡æ»¤å¹²æ‰°åè®®ä¸ç©ºç‰©è´¨
            if not isinstance(url, str) or not url or url.startswith(('javascript:', 'place:', 'data:')):
                continue

            title = link.get_text().strip() or "Untitled Signal"
            
            # èƒå–æ ¸å¿ƒä¿¡å·åˆ‡ç‰‡
            signal = {
                "title": title,
                "url": url,
                "context": self._extract_context(link),
                "timestamp": self._normalize_timestamp(str(link.get('add_date', ''))),
                "tags": []
            }
            
            # æå–æµè§ˆå™¨åŸç”Ÿæ ‡ç­¾ (å¦‚æœå­˜åœ¨)
            # å…¼å®¹ TAGS (å¤§å†™) å’Œ tags (å°å†™)
            raw_tags = link.get('tags') or link.get('TAGS')
            if isinstance(raw_tags, str):
                signal["tags"] = [t.strip() for t in raw_tags.split(',')]

            purified_data.append(signal)

        return purified_data

    def process_file(self, input_path: str) -> List[Dict[str, Any]]:
        """
        [çœŸç†å…¥å£]: éªŒè¯è½½ä½“å¹¶æ‰§è¡ŒäºŒè¿›åˆ¶ç†”ç‚¼ã€‚
        """
        if not os.path.isfile(input_path):
            raise FileNotFoundError(f"âŒ [Error] è½½ä½“ç¼ºå¤±: {input_path}")

        print(f"âš—ï¸  [Athanor] æ­£åœ¨å¼€å¯ç†”ç‚‰ï¼ŒåŠ è½½åŸæ–™: {os.path.basename(input_path)}")
        
        try:
            with open(input_path, 'rb') as f:
                raw_content = f.read()
            return self.smelt(raw_content)
        except IOError as e:
            print(f"âŒ [Error] ç‰©è´¨è¯»å…¥å¤±è´¥: {e}")
            return []

# --- éªŒè¯é€»è¾‘ ---
def main():
    # ç¯å¢ƒæ„ŸçŸ¥è·¯å¾„ï¼šæ”¯æŒç¯å¢ƒå˜é‡æ³¨å…¥ï¼Œå¦åˆ™é™çº§è‡³é»˜è®¤æµ‹è¯•è·¯å¾„
    target_path = os.getenv("ATHANOR_INPUT", "data/bookmarks_raw.html")
    
    purifier = AthanorPurifier()
    
    try:
        results = purifier.process_file(target_path)
        print(f"âœ¨  [Success] èƒå–å®Œæˆï¼Œè·å¾— {len(results)} æ¡é«˜çº¯åº¦ä¿¡å·ã€‚")
        
        # æ£€è§†å‰ 5 æ¡èƒå–å‡ºçš„ä¿¡å·
        for s in results[:5]:
            path_str = " > ".join(s['context']) if s['context'] else "ROOT"
            print(f"[{s['timestamp']}] {path_str} | {s['title']} -> {s['url'][:50]}...")
            
    except Exception as e:
        print(f"âŒ [Critical] ç†”ç‚¼äº‹æ•…: {e}")

if __name__ == "__main__":
    main()