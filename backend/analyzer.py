import numpy as np
from collections import defaultdict
from typing import List, Dict, Any, Optional

# å‚¬åŒ–å‰‚æ£€æŸ¥ï¼šç¡®ä¿ç‚¼é‡‘è¯•å‰‚å­˜åœ¨
try:
    import jieba
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    DEPENDENCIES_INSTALLED = True
except ImportError:
    DEPENDENCIES_INSTALLED = False

class KnowledgeCrystallizer:
    """
    [ç‚¼é‡‘ç»„ä»¶]: çŸ¥è¯†ç»“æ™¶å™¨ (V1 ç»“æ™¶ç‰ˆ)
    é©±åŠ¨å¼•æ“: Scikit-learn + Jieba
    é€»è¾‘: TF-IDF èƒå– -> K-Means èšåˆ -> é‡å¿ƒé€†å‘æ ‡è®°
    """

    def __init__(self, n_clusters: int = 8):
        self.n_clusters = n_clusters
        # ğŸ›¡ï¸ å™ªéŸ³å±è”½åœº (Stop Words)
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'æ•™ç¨‹', 'æŒ‡å—', '2023', '2024', '2025', 'com', 'cn', 'net', 'org', 'github', 'å®˜ç½‘', 'ä¸‹è½½', 'ä½¿ç”¨', 'æ–¹æ³•', 'è§£å†³', 'æ¨è', 'å·¥å…·', 'å¹³å°'
        }

    def _tokenize(self, text: str) -> str:
        """åŸå­çº§åˆ†è¯ï¼šå°†æ–‡æœ¬æ‰“æ•£ä¸ºè¯­ä¹‰ç²‰æœ«"""
        if not text: return ""
        # ä½¿ç”¨ lcut ç›´æ¥è·å–åˆ—è¡¨ï¼Œå‡å°‘ç”Ÿæˆå™¨ä¸Šä¸‹æ–‡å¼€é”€
        words = jieba.lcut(text)
        # è¿‡æ»¤é•¿åº¦ä¸º1çš„å•å­—ï¼ˆé€šå¸¸æ˜¯å™ªéŸ³ï¼‰å’Œåœç”¨è¯
        return " ".join([w for w in words if len(w) > 1 and w not in self.stop_words])

    def crystallize(self, bookmarks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [ç†”ç‚¼æµç¨‹]: æ‰§è¡Œå…‰è°±èšç±»å¹¶æå‡ºæ˜Ÿç¾¤ç»“æ™¶
        """
        if not DEPENDENCIES_INSTALLED:
            print("âŒ [Crystallizer] ç¼ºå°‘å…³é”®è¯•å‰‚: sklearn æˆ– jiebaã€‚")
            return []

        # 1. åŸæ–™ç­›é€‰
        corpus = []
        valid_items = []
        for b in bookmarks:
            tokenized = self._tokenize(b.get('title', ''))
            if len(tokenized.split()) >= 2: # è‡³å°‘ä¿ç•™2ä¸ªè¯­ä¹‰ç‰¹å¾çš„ä¿¡å·
                corpus.append(tokenized)
                valid_items.append(b)

        # ğŸ›¡ï¸ æ ·æœ¬ä¿æŠ¤ï¼šå¦‚æœæ ·æœ¬é‡å¤ªå°‘ï¼Œèšç±»ä¼šé€€åŒ–ä¸ºå™ªéŸ³
        min_samples = self.n_clusters * 2
        if len(corpus) < min_samples:
            print(f"âš ï¸ [Crystallizer] æ ·æœ¬é‡ ({len(corpus)}) ä¸è¶³ï¼Œæ˜Ÿç¾¤æ— æ³•æå‡ºã€‚")
            return []

        print(f"âš—ï¸ [Crystallizer] æ­£åœ¨é«˜ç»´ç©ºé—´è§£æ {len(corpus)} æ¡çŸ¥è¯†è·¯å¾„...")

        # 2. å‘é‡åŒ– (TF-IDF)
        # token_pattern=r"(?u)\b\w+\b" ç¡®ä¿å…¼å®¹ä¸­è‹±æ··åˆ
        vectorizer = TfidfVectorizer(max_features=1000, token_pattern=r"(?u)\b\w+\b")
        try:
            X = vectorizer.fit_transform(corpus)
        except Exception as e:
            print(f"âŒ [Crystallizer] å‘é‡åŒ–å¤±è´¥: {e}")
            return []

        # 3. ç©ºé—´èšç±» (K-Means)
        # ä½¿ç”¨ random_state=42 ç¡®ä¿æ¯æ¬¡ç‚¼é‡‘çš„ç¨³å®šæ€§
        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto')
        kmeans.fit(X)

        # 4. é€†å‘æå‡ºæ˜Ÿç¾¤æ ‡ç­¾
        feature_names = vectorizer.get_feature_names_out()
        cluster_map = defaultdict(list)
        for idx, label in enumerate(kmeans.labels_):
            cluster_map[label].append(valid_items[idx])

        crystals = []
        for i in range(self.n_clusters):
            items = cluster_map[i]
            if not items: continue

            # é€šè¿‡èšç±»é‡å¿ƒ (Centroid) é€†å‘è·å–æœ€é‡è¦çš„ 3 ä¸ªç‰¹å¾è¯
            centroid = kmeans.cluster_centers_[i]
            top_indices = centroid.argsort()[-3:][::-1]
            keywords = [feature_names[idx] for idx in top_indices]
            
            cluster_name = " + ".join(keywords).upper()

            crystals.append({
                "cluster_id": i,
                "topic": cluster_name,
                "size": len(items),
                "nodes": items[:10], # åªä¿ç•™å‰10ä¸ªæ ·æœ¬ä½œä¸ºé¢„è§ˆä¿¡å·
                "keywords": keywords
            })

        # æŒ‰æ˜Ÿç¾¤å¼•åŠ›ï¼ˆå¤§å°ï¼‰é™åºæ’åˆ—
        return sorted(crystals, key=lambda x: x['size'], reverse=True)

    def analyze_timeline(self, bookmarks: List[Dict[str, Any]]) -> Dict[str, int]:
        """[Chronos]: æ—¶é—´çƒ­åŠ›å›¾æå‡º"""
        counts = defaultdict(int)
        for b in bookmarks:
            ts = b.get('timestamp', '')
            if ts and len(ts) >= 7:
                counts[ts[:7]] += 1 # ç»Ÿè®¡ YYYY-MM
        return dict(sorted(counts.items()))

# --- éªŒè¯é€»è¾‘ ---
if __name__ == "__main__":
    crystallizer = KnowledgeCrystallizer(n_clusters=3)
    # æ³¨å…¥æµ‹è¯•åŸæ–™...