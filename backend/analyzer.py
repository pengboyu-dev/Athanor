from collections import defaultdict
from typing import List, Dict, Any, Optional

# å‚¬åŒ–å‰‚æ£€æŸ¥ï¼šç¡®ä¿ç‚¼é‡‘è¯•å‰‚å­˜åœ¨
try:
    import jieba
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    DEPENDENCIES_INSTALLED = True
except ImportError:
    DEPENDENCIES_INSTALLED = False

class KnowledgeCrystallizer:
    """
    [ç‚¼é‡‘ç»„ä»¶]: çŸ¥è¯†ç»“æ™¶å™¨ (V1 ç»“æ™¶ç‰ˆ)
    é©±åŠ¨å¼•æ“: Scikit-learn + Jieba
    é€»è¾‘: TF-IDF èƒå– -> K-Means èšåˆ -> é‡å¿ƒé€†å‘æ ‡è®°
    """

    def __init__(self, n_clusters: int = 8):
        self.n_clusters = n_clusters
        # ğŸ›¡ï¸ å™ªéŸ³å±è”½åœº (Stop Words)
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'æ•™ç¨‹', 'æŒ‡å—', '2023', '2024', '2025', 'com', 'cn', 'net', 'org', 'github', 'å®˜ç½‘', 'ä¸‹è½½', 'ä½¿ç”¨', 'æ–¹æ³•', 'è§£å†³', 'æ¨è', 'å·¥å…·', 'å¹³å°'
        }

    def _tokenize(self, text: str) -> str:
        """åŸå­çº§åˆ†è¯ï¼šå°†æ–‡æœ¬æ‰“æ•£ä¸ºè¯­ä¹‰ç²‰æœ«"""
        if not text: return ""
        # ä½¿ç”¨ lcut ç›´æ¥è·å–åˆ—è¡¨ï¼Œå‡å°‘ç”Ÿæˆå™¨ä¸Šä¸‹æ–‡å¼€é”€
        words = jieba.lcut(text)
        # è¿‡æ»¤é•¿åº¦ä¸º1çš„å•å­—ï¼ˆé€šå¸¸æ˜¯å™ªéŸ³ï¼‰å’Œåœç”¨è¯
        return " ".join([w for w in words if len(w) > 1 and w not in self.stop_words])

    def crystallize(self, bookmarks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        [ç†”ç‚¼æµç¨‹]: æ‰§è¡Œå…‰è°±èšç±»å¹¶æå‡ºæ˜Ÿç¾¤ç»“æ™¶
        """
        if not DEPENDENCIES_INSTALLED:
            print("âŒ [Crystallizer] ç¼ºå°‘å…³é”®è¯•å‰‚: sklearn æˆ– jiebaã€‚")
            return []

        # 1. åŸæ–™ç­›é€‰
        corpus = []
        valid_items = []
        for b in bookmarks:
            tokenized = self._tokenize(b.get('title', ''))
            if len(tokenized.split()) >= 2: # è‡³å°‘ä¿ç•™2ä¸ªè¯­ä¹‰ç‰¹å¾çš„ä¿¡å·
                corpus.append(tokenized)
                valid_items.append(b)

        # ğŸ›¡ï¸ æ ·æœ¬ä¿æŠ¤ï¼šå¦‚æœæ ·æœ¬é‡å¤ªå°‘ï¼Œèšç±»ä¼šé€€åŒ–ä¸ºå™ªéŸ³
        min_samples = self.n_clusters * 2
        if len(corpus) < min_samples:
            print(f"âš ï¸ [Crystallizer] æ ·æœ¬é‡ ({len(corpus)}) ä¸è¶³ï¼Œæ˜Ÿç¾¤æ— æ³•æå‡ºã€‚")
            return []

        print(f"âš—ï¸ [Crystallizer] æ­£åœ¨é«˜ç»´ç©ºé—´è§£æ {len(corpus)} æ¡çŸ¥è¯†è·¯å¾„...")

        # 2. å‘é‡åŒ– (TF-IDF)
        # token_pattern=r"(?u)\b\w+\b" ç¡®ä¿å…¼å®¹ä¸­è‹±æ··åˆ
        vectorizer = TfidfVectorizer(max_features=1000, token_pattern=r"(?u)\b\w+\b")
        try:
            X = vectorizer.fit_transform(corpus)
        except Exception as e:
            print(f"âŒ [Crystallizer] å‘é‡åŒ–å¤±è´¥: {e}")
            return []

        # 3. ç©ºé—´èšç±» (K-Means)
        # ä½¿ç”¨ random_state=42 ç¡®ä¿æ¯æ¬¡ç‚¼é‡‘çš„ç¨³å®šæ€§
        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto')
        kmeans.fit(X)

        # 4. é€†å‘æå‡ºæ˜Ÿç¾¤æ ‡ç­¾
        feature_names = vectorizer.get_feature_names_out()
        cluster_map = defaultdict(list)
        for idx, label in enumerate(kmeans.labels_):
            cluster_map[label].append(valid_items[idx])

        crystals = []
        for i in range(self.n_clusters):
            items = cluster_map[i]
            if not items: continue

            # é€šè¿‡èšç±»é‡å¿ƒ (Centroid) é€†å‘è·å–æœ€é‡è¦çš„ 3 ä¸ªç‰¹å¾è¯
            centroid = kmeans.cluster_centers_[i]
            top_indices = centroid.argsort()[-3:][::-1]
            keywords = [feature_names[idx] for idx in top_indices]
            
            cluster_name = " + ".join(keywords).upper()

            crystals.append({
                "cluster_id": i,
                "topic": cluster_name,
                "size": len(items),
                "nodes": items[:10], # åªä¿ç•™å‰10ä¸ªæ ·æœ¬ä½œä¸ºé¢„è§ˆä¿¡å·
                "keywords": keywords
            })

        # æŒ‰æ˜Ÿç¾¤å¼•åŠ›ï¼ˆå¤§å°ï¼‰é™åºæ’åˆ—
        return sorted(crystals, key=lambda x: x['size'], reverse=True)

    def analyze_timeline(self, bookmarks: List[Dict[str, Any]]) -> Dict[str, int]:
        """[Chronos]: æ—¶é—´çƒ­åŠ›å›¾æå‡º"""
        counts = defaultdict(int)
        for b in bookmarks:
            ts = b.get('timestamp', '')
            if ts and len(ts) >= 7:
                counts[ts[:7]] += 1 # ç»Ÿè®¡ YYYY-MM
        return dict(sorted(counts.items()))

    def analyze_domains(self, bookmarks: List[Dict[str, Any]], top_n: int = 10) -> List[Dict[str, Any]]:
        """[Territory]: åŸŸåé¢†åœ°åˆ†æ"""
        from urllib.parse import urlparse
        domain_counts = defaultdict(int)
        
        for b in bookmarks:
            url = b.get('url', '')
            try:
                domain = urlparse(url).netloc
                if domain.startswith('www.'):
                    domain = domain[4:]
                if domain:
                    domain_counts[domain] += 1
            except:
                continue
                
        sorted_domains = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]
        return [{"name": d, "value": c} for d, c in sorted_domains]

    def analyze_activity_hours(self, bookmarks: List[Dict[str, Any]]) -> Dict[str, int]:
        """[Circadian]: æ˜¼å¤œæ´»è·ƒèŠ‚å¾‹"""
        hours = defaultdict(int)
        for b in bookmarks:
            ts = b.get('timestamp', '') # YYYY-MM-DD HH:MM:SS
            if ts and len(ts) >= 13:
                hour = ts[11:13] # Extract HH
                hours[hour] += 1
        return dict(sorted(hours.items()))

    def analyze_skill_radar(self, bookmarks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """[Skill Tree]: æŠ€èƒ½å…­è¾¹å½¢é›·è¾¾æ•°æ®"""
        # å®šä¹‰æŠ€èƒ½ç»´åº¦ä¸å…³é”®è¯æ˜ å°„
        dimensions = {
            "Coding": ["github", "stackoverflow", "csdn", "juejin", "python", "java", "code", "git", "api", "dev"],
            "AI/ML": ["arxiv", "huggingface", "openai", "gpt", "model", "deep", "learning", "ai", "bot"],
            "Product": ["figma", "dribbble", "producthunt", "notion", "linear", "design", "ui", "ux"],
            "Media": ["bilibili", "youtube", "netflix", "spotify", "music", "video", "douban", "movie"],
            "Academic": ["scholar", "edu", "university", "paper", "research", "science", "wiki", "book"],
            "Life": ["taobao", "jd", "amazon", "map", "food", "travel", "news", "blog"]
        }
        
        scores = defaultdict(int)
        
        for b in bookmarks:
            content = (b.get('title', '') + " " + b.get('url', '')).lower()
            for dim, keywords in dimensions.items():
                for kw in keywords:
                    if kw in content:
                        scores[dim] += 1
                        break # ä¸€ä¸ªä¹¦ç­¾åœ¨ä¸€ä¸ªç»´åº¦åªç®—ä¸€æ¬¡
        
        # å½’ä¸€åŒ–å¤„ç†ï¼šæ‰¾å‡ºæœ€å¤§å€¼ï¼Œå°†æ‰€æœ‰å€¼æ˜ å°„åˆ° 0-100
        max_score = max(scores.values()) if scores else 1
        
        radar_data = []
        for dim in dimensions.keys():
            raw_score = scores[dim]
            # ç®€å•çš„çº¿æ€§æ˜ å°„ï¼Œä½†ä¹Ÿä¿ç•™åŸå§‹å€¼
            normalized = int((raw_score / max_score) * 100) if max_score > 0 else 0
            radar_data.append({
                "name": dim,
                "value": raw_score, 
                "max": max_score + 5 # é›·è¾¾å›¾çš„æœ€å¤§åˆ»åº¦ç•¥å¤§äºå®é™…æœ€å¤§å€¼
            })
            
        return radar_data

    def generate_persona(self, bookmarks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """[Persona]: ç”¨æˆ·ç”»åƒç”Ÿæˆå™¨ (V3 æ ¸å¿ƒ)"""
        if not bookmarks:
            return {"level": "Lv.0 èŒæ–°", "tags": [], "top_domain": "N/A", "top_cluster": "N/A"}

        # 1. è®¡ç®—ç­‰çº§ (åŸºäºæ”¶è—é‡)
        count = len(bookmarks)
        if count < 100: level = "Lv.1 æ¢ç´¢è€…"
        elif count < 500: level = "Lv.2 æ”¶è—å®¶"
        elif count < 1000: level = "Lv.3 çŸ¥è¯†å›¤ç§¯è€…"
        elif count < 5000: level = "Lv.4 å›¾ä¹¦é¦†é•¿"
        else: level = "Lv.5 èµ›åšè´¤è€…"

        # 2. æå–æ ‡ç­¾ (åŸºäºåŸŸåè§„åˆ™)
        tags = set()
        from urllib.parse import urlparse
        domains = []
        for b in bookmarks:
            try:
                d = urlparse(b.get('url', '')).netloc
                domains.append(d)
            except: pass
        
        domain_str = " ".join(domains).lower()
        
        # è§„åˆ™å¼•æ“
        if "github.com" in domain_str or "stackoverflow.com" in domain_str: tags.add("å¼€æºæå®¢")
        if "bilibili.com" in domain_str or "youtube.com" in domain_str: tags.add("è§†å¬å­¦ä¹ è€…")
        if "arxiv.org" in domain_str or "scholar.google" in domain_str: tags.add("å­¦æœ¯ç ”ç©¶")
        if "zhihu.com" in domain_str or "medium.com" in domain_str: tags.add("æ·±åº¦é˜…è¯»")
        if "taobao.com" in domain_str or "jd.com" in domain_str: tags.add("æ•°å­—ç”Ÿæ´»")
        if "figma.com" in domain_str or "dribbble.com" in domain_str: tags.add("è®¾è®¡ç¾å­¦")

        # 3. è·å– Top 1 åŸŸå (æœ€çˆ±æ¥æº)
        domain_stats = self.analyze_domains(bookmarks, top_n=1)
        top_domain = domain_stats[0]['name'] if domain_stats else "N/A"

        # 4. è·å– Top 1 èšç±» (ä¸“æ³¨é¢†åŸŸ)
        # è¿™é‡Œéœ€è¦å¤ç”¨å·²ç»ç”Ÿæˆçš„ç»“æ™¶æ•°æ®ï¼Œä¸ºäº†è§£è€¦ï¼Œæˆ‘ä»¬ç®€å•ç”¨è¯é¢‘æœ€é«˜çš„è¯ä»£æ›¿ï¼Œæˆ–è€…ç”±ä¸Šå±‚ä¼ å…¥
        # æš‚æ—¶ç”¨è¯é¢‘æœ€é«˜çš„éåœç”¨è¯ä½œä¸º"ä¸“æ³¨é¢†åŸŸ"
        cloud = self.analyze_tags_cloud(bookmarks, top_n=1)
        top_cluster = cloud[0]['name'] if cloud else "æ‚å­¦å®¶"

        return {
            "level": level,
            "tags": list(tags)[:3], # æœ€å¤šå±•ç¤º3ä¸ªæ ‡ç­¾
            "top_domain": top_domain,
            "top_cluster": top_cluster
        }


    def analyze_theme_river(self, bookmarks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """[River]: å…´è¶£æ²³æµå›¾ (ThemeRiver) æ•°æ®"""
        # æŒ‰æœˆèšåˆï¼Œç»Ÿè®¡æ¯ä¸ªæœˆçš„ Top 3 èšç±»è¯é¢‘
        # 1. å…ˆæŒ‰æœˆåˆ†ç»„
        monthly_data = defaultdict(list)
        for b in bookmarks:
            ts = b.get('timestamp', '')
            if ts and len(ts) >= 7:
                month = ts[:7] # YYYY-MM
                monthly_data[month].append(b)
        
        # 2. å¯¹æ¯ä¸ªæœˆçš„æ•°æ®æå–å…³é”®è¯
        river_data = []
        months = sorted(monthly_data.keys())
        
        # ä¸ºäº†ä¿æŒæ²³æµçš„è¿è´¯æ€§ï¼Œæˆ‘ä»¬é€‰å–å…¨å±€æœ€é«˜é¢‘çš„ 5 ä¸ªè¯ä½œä¸º"æ²³é“"
        global_cloud = self.analyze_tags_cloud(bookmarks, top_n=5)
        top_themes = [item['name'] for item in global_cloud]
        
        for month in months:
            items = monthly_data[month]
            # ç»Ÿè®¡è¯¥æœˆå†…è¿™5ä¸ªä¸»é¢˜è¯çš„é¢‘ç‡
            month_counts = defaultdict(int)
            for b in items:
                text = b.get('title', '')
                for theme in top_themes:
                    if theme in text:
                        month_counts[theme] += 1
            
            for theme in top_themes:
                river_data.append({
                    "date": month,
                    "name": theme,
                    "value": month_counts[theme]
                })
                
        return river_data

    def analyze_tags_cloud(self, bookmarks: List[Dict[str, Any]], top_n: int = 50) -> List[Dict[str, Any]]:
        """[Nebula]: è¯­ä¹‰æ˜Ÿäº‘ï¼ˆè¯äº‘æ•°æ®ï¼‰"""
        if not DEPENDENCIES_INSTALLED: return []
        
        word_counts = defaultdict(int)
        for b in bookmarks:
            # ç»“åˆæ ‡é¢˜å’Œç°æœ‰æ ‡ç­¾
            text = b.get('title', '') + " " + " ".join(b.get('tags', []))
            words = self._tokenize(text).split()
            for w in words:
                word_counts[w] += 1
                
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]
        return [{"name": w, "value": c} for w, c in sorted_words]

# --- éªŒè¯é€»è¾‘ ---
if __name__ == "__main__":
    crystallizer = KnowledgeCrystallizer(n_clusters=3)
    # æ³¨å…¥æµ‹è¯•åŸæ–™...